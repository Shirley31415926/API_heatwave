{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shirley31415926/API_heatwave/blob/main/08%20Topic%20Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hSQjGiq8FQcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic[all] -q\n",
        "!pip install sentence-transformers -q"
      ],
      "metadata": {
        "id": "q-uRdy7fFKwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTopic input\n",
        "!pip install bertopic\n",
        "!pip install plotly\n",
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "1J0lWT07Lw0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2023 Topic modelling"
      ],
      "metadata": {
        "id": "GA9OsDhsucbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic umap-learn plotly nltk sentence-transformers -q\n",
        "\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic\n",
        "import plotly.io as pio\n",
        "\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "STOP_WORDS = stopwords.words(\"english\")\n",
        "\n",
        "###Load & preprocess data\n",
        "DATA_PATH = \"/content/drive/MyDrive/heat_posts2023.jsonl\"\n",
        "posts_df = pd.read_json(DATA_PATH, lines=True)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Lowercase, remove URLs and non-alpha characters.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
        "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "# Apply cleaning and parse dates\n",
        "posts_df[\"clean_text\"] = posts_df[\"text\"].apply(clean_text)\n",
        "posts_df[\"date\"]       = pd.to_datetime(posts_df[\"created_at\"]).dt.date\n",
        "\n",
        "texts = posts_df[\"clean_text\"].tolist()\n",
        "dates = posts_df[\"date\"].tolist()\n",
        "\n",
        "###Fit BERTopic model\n",
        "# Using a smaller min_topic_size so even tiny clusters survive\n",
        "topic_model = BERTopic(\n",
        "    embedding_model   = SentenceTransformer(\"all-MiniLM-L6-v2\"),\n",
        "    vectorizer_model  = CountVectorizer(stop_words=STOP_WORDS, ngram_range=(1,2)),\n",
        "    language          = \"english\",\n",
        "    calculate_probabilities = True,\n",
        "    verbose           = True,\n",
        "    min_topic_size    = 2,\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(texts)\n",
        "posts_df[\"topic\"] = topics\n",
        "\n",
        "###Auto‐generate summaries for each topic\n",
        "summary_map = {}\n",
        "for t in topic_model.get_topic_info().Topic:\n",
        "    if t == -1:\n",
        "        continue\n",
        "    top2 = [w for w,_ in topic_model.get_topic(t)[:2]]\n",
        "    summary_map[t] = \"_\".join(top2)\n",
        "\n",
        "posts_df[\"topic_summary\"] = posts_df[\"topic\"].map(lambda t: summary_map.get(t, \"Other\"))\n",
        "\n",
        "\n",
        "print(\"Topic summaries:\")\n",
        "for t, label in summary_map.items():\n",
        "    print(f\"  {t:2d} → {label}\")\n",
        "\n",
        "###“Health / Energy / Policy / Climate” mapping\n",
        "label_dict = {\n",
        "    \"Health\":  [\"heatstroke\",\"hospital\",\"dehydration\",\"emergency\",\"risk\",\"dizzy\",\"overheat\"],\n",
        "    \"Energy\":  [\"power\",\"outage\",\"electricity\",\"grid\",\"air conditioning\",\"cooling\",\"blackout\"],\n",
        "    \"Policy\":  [\"government\",\"response\",\"support\",\"aid\",\"fund\",\"relief\",\"governor\"],\n",
        "    \"Climate\": [\"climate\",\"crisis\",\"warming\",\"carbon\",\"emission\",\"anxiety\"]\n",
        "}\n",
        "\n",
        "def assign_label(keywords, mapping):\n",
        "    keys = set(w for w,_ in keywords)\n",
        "    scores = {lbl: len(keys & set(words)) for lbl,words in mapping.items()}\n",
        "    return max(scores, key=scores.get) if max(scores.values())>0 else \"Other\"\n",
        "\n",
        "topic_label_map = {\n",
        "    t: assign_label(topic_model.get_topic(t), label_dict)\n",
        "    for t in summary_map.keys()\n",
        "}\n",
        "posts_df[\"semantic_label\"] = posts_df[\"topic\"].map(lambda t: topic_label_map.get(t, \"Other\"))\n",
        "\n",
        "###Visualize topic‐summary trends over time\n",
        "time_df = posts_df.groupby([\"date\",\"topic_summary\"]).size().unstack(fill_value=0)\n",
        "# pick top 10 summaries by overall counts\n",
        "top10 = time_df.sum().sort_values(ascending=False).head(50).index\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "for lbl in top10:\n",
        "    plt.plot(time_df.index, time_df[lbl], marker=\".\", label=lbl)\n",
        "\n",
        "plt.title(\"Top-10 BERTopic Summaries Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Post Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Topic Summary\", bbox_to_anchor=(1.02,1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "###results\n",
        "posts_df.to_csv(\"heatwave_posts_with_topics.csv\", index=False)\n",
        "pd.DataFrame(topic_model.topics_over_time(texts, dates)).to_csv(\"topics_over_time.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "zCcpVnFLlh9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_barchart(top_n_topics=35).show()"
      ],
      "metadata": {
        "id": "3ITrBj-enB2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2024 Topic modelling"
      ],
      "metadata": {
        "id": "tnGu64x7uh0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic umap-learn plotly nltk sentence-transformers -q\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic import BERTopic\n",
        "import plotly.io as pio\n",
        "\n",
        "\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "STOP_WORDS = stopwords.words(\"english\")\n",
        "\n",
        "###Load data\n",
        "DATA_PATH = \"/content/drive/MyDrive/heat_posts2024_standardized.jsonl\"\n",
        "posts_df = pd.read_json(DATA_PATH, lines=True)\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Lowercase, remove URLs and non-alpha characters.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
        "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "posts_df[\"clean_text\"] = posts_df[\"text\"].apply(clean_text)\n",
        "posts_df[\"date\"]       = pd.to_datetime(posts_df[\"created_at\"]).dt.date\n",
        "\n",
        "texts = posts_df[\"clean_text\"].tolist()\n",
        "dates = posts_df[\"date\"].tolist()\n",
        "\n",
        "\n",
        "### Fit BERTopic model\n",
        "# Using a smaller min_topic_size so even tiny clusters survive\n",
        "topic_model = BERTopic(\n",
        "    embedding_model   = SentenceTransformer(\"all-MiniLM-L6-v2\"),\n",
        "    vectorizer_model  = CountVectorizer(stop_words=STOP_WORDS, ngram_range=(1,2)),\n",
        "    language          = \"english\",\n",
        "    calculate_probabilities = True,\n",
        "    verbose           = True,\n",
        "    min_topic_size    = 2,\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(texts)\n",
        "posts_df[\"topic\"] = topics\n",
        "\n",
        "\n",
        "#  Auto‐generate summaries for each topic\n",
        "summary_map = {}\n",
        "for t in topic_model.get_topic_info().Topic:\n",
        "    if t == -1:\n",
        "        continue\n",
        "    top2 = [w for w,_ in topic_model.get_topic(t)[:2]]\n",
        "    summary_map[t] = \"_\".join(top2)\n",
        "\n",
        "posts_df[\"topic_summary\"] = posts_df[\"topic\"].map(lambda t: summary_map.get(t, \"Other\"))\n",
        "\n",
        "print(\"Topic summaries:\")\n",
        "for t, label in summary_map.items():\n",
        "    print(f\"  {t:2d} → {label}\")\n",
        "\n",
        "# “Health / Energy / Policy / Climate” mapping\n",
        "label_dict = {\n",
        "    \"Health\":  [\"heatstroke\",\"hospital\",\"dehydration\",\"emergency\",\"risk\",\"dizzy\",\"overheat\"],\n",
        "    \"Energy\":  [\"power\",\"outage\",\"electricity\",\"grid\",\"air conditioning\",\"cooling\",\"blackout\"],\n",
        "    \"Policy\":  [\"government\",\"response\",\"support\",\"aid\",\"fund\",\"relief\",\"governor\"],\n",
        "    \"Climate\": [\"climate\",\"crisis\",\"warming\",\"carbon\",\"emission\",\"anxiety\"]\n",
        "}\n",
        "\n",
        "def assign_label(keywords, mapping):\n",
        "    keys = set(w for w,_ in keywords)\n",
        "    scores = {lbl: len(keys & set(words)) for lbl,words in mapping.items()}\n",
        "    return max(scores, key=scores.get) if max(scores.values())>0 else \"Other\"\n",
        "\n",
        "topic_label_map = {\n",
        "    t: assign_label(topic_model.get_topic(t), label_dict)\n",
        "    for t in summary_map.keys()\n",
        "}\n",
        "posts_df[\"semantic_label\"] = posts_df[\"topic\"].map(lambda t: topic_label_map.get(t, \"Other\"))\n",
        "\n",
        "### visualize topic‐summary trends over time\n",
        "# pivot\n",
        "time_df = posts_df.groupby([\"date\",\"topic_summary\"]).size().unstack(fill_value=0)\n",
        "# pick top 10 summaries by overall counts\n",
        "top10 = time_df.sum().sort_values(ascending=False).head(50).index\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "for lbl in top10:\n",
        "    plt.plot(time_df.index, time_df[lbl],marker=\".\", label=lbl)\n",
        "\n",
        "plt.title(\"Top-10 BERTopic Summaries Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Post Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title=\"Topic Summary\", bbox_to_anchor=(1.02,1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "### results\n",
        "posts_df.to_csv(\"heatwave_posts_with_topics.csv\", index=False)\n",
        "pd.DataFrame(topic_model.topics_over_time(texts, dates)).to_csv(\"topics_over_time.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "66K1b2KTmya4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_barchart(top_n_topics=35).show()"
      ],
      "metadata": {
        "id": "tuTciCjUqNJj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}